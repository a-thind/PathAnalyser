#' Accuracy calculation of classification method
#' @description Accuracy calculation is performed using classified labels from the classification method and actual labels.
#' Confusion matrix is created to display the accuracy to the user. Additional statistical information and ROC curve are
#' optional features. It should be called after classificayion method to evaluate accuracy of classification
#' @author Ozlem Karadeniz \email{ozlem.karadeniz.283@cranfield.ac.uk}
#' @param true_labels_source  dataframe ,matrix or name of file which contains sample Ids and corresponding actual labels.
#' @param predicted_labels_source predicted labels dataframe or matrix generated by classification method
#' @param pathway name of pathway, currently HER2 and ER are defined.
#' @param display_statistics optional flag to display statisctival information about confusion matrix
#' @param display_roc_curve optional flag to plot ROC Curve for confusion matrix
#' @return confusion_matrix
#' @export
#'
#' @examples
#' calculate_accuracy(true_labels_df,predicted_labels_df, "ER", display_statistics= TRUE, display_roc_curve=TRUE)

calculate_accuracy <-function(true_labels_source, predicted_labels_source, pathway,
                              display_statistics=FALSE, display_roc_curve=FALSE){
  # if type of true_labels_source parameter is character,then it should be file name.
  # file is checked if it exists and valid
  if(is.character(true_labels_source)){
    if(!file.exists(true_labels_source)){
      stop(paste("Error in calculate_accuracy: " , true_labels_source , " file does not exist!"))
    }
    else{
      true_labels_df <- tryCatch(read.table(true_labels_source, header=TRUE,sep = "\t"),
                                 error=function(e){ stop(paste("File read error in calculate_accuracy: filename = ", true_labels_source , "!")) }
      )
    }
  }
  # true_labels_source is checked if it is dataframe or matrix
  # error is thrown if true_labels_source parameter is not compatible
  else if (is.matrix(true_labels_df)){
    true_labels_df <- as.data.frame(true_labels_df)
  }
  else if (!is.data.frame(true_labels_df)){
    stop("Error in calculate_accuracy: true_labels_source argument should be of type dataframe/matrix/file name!")
  }


  # predicted_labels_source is checked if it is dataframe or matrix
  # error is thrown if predicted_labels_source parameter is not compatible
  if (is.matrix(predicted_labels_source)){
    predicted_labels_df <- as.data.frame(predicted_labels_source)
  }

  else if (!is.data.frame(predicted_labels_source)){
    stop("Error in calculate_accuracy: predicted_labels_source argument should be of type dataframe/matrix!")
  }
  else{
    predicted_labels_df <- predicted_labels_source
  }

  # Currently "HER2", "ER" pathways are defined in the method.
  # Error is thrown if any undefined pathway parameter has been provided
  if((pathway %in% c("HER2", "ER")) == FALSE){
    stop(paste("Error in calculate_accuracy: pathway argument ", pathway , " is undefined. It should be HER2 or ER!"))
  }

  # Only labels for the label is kept in  true_labels_df, the rest is removed
  if(pathway == "HER2"){
    true_labels_df =  true_labels_df[, which(colnames(true_labels_df)=="ER") * -1]
  }else{
    true_labels_df <- true_labels_df[, which(colnames(true_labels_df)=="HER2") * -1]
  }

  # Labels Equivocal, [Not Evaluated] and Indeterminate are all set to uncertain
  true_labels_df[,2] <- sapply(true_labels_df[,2],
                               function(x){gsub(pattern = "(Equivocal|\\[Not Evaluated\\]|Indeterminate)",
                                                replacement = "Uncertain", x)})

  # label name is updated to label, in order to have the same for both HER AND ER pathways
  colnames(true_labels_df)[2] <- "label"

  # predicted_labels_df.sample and true_labels_df.CaseID are set to the same format
  predicted_labels_df[,1] <- gsub("\\.", "-", predicted_labels_df[,1])

  # df dataframe created which holds sample name(caseID), actual label(label) and predictedl label(class)
  df<-merge(x=true_labels_df, y=predicted_labels_df, by.x = "CaseID", by.y="sample")

  # statistical data is calculated below to create confusion matrix
  # True Positive, predicted positive, actual positive
  TP <- nrow(df[(df$class == "Active" & df$label == "Positive"),])
  # False Positive, predicted positive, actual negative
  FP <- nrow(df[(df$class == "Active" & df$label == "Negative"),])
  # True Negative, predicted negative, actual negative
  TN <- nrow(df[(df$class == "Inactive" & df$label == "Negative"),])
  # False Negative, predicted negative, actual positive
  FN <- nrow(df[(df$class == "Inactive" & df$label == "Positive"),])
  # predicted uncertain, actual positive
  prd_uncert_act_positive = nrow(df[(df$class == "Uncertain" & df$label == "Positive"),])
  # predicted uncertain, actual negative
  prd_uncert_act_negative = nrow(df[(df$class == "Uncertain" & df$label == "Negative"),])
  # predicted positive, actual uncertain
  prd_positive_act_uncertain = nrow(df[(df$class == "Active" & df$label == "Uncertain"),])
  # predicted negative, actual uncertain
  prd_negative_act_uncertain = nrow(df[(df$class == "Inactive" & df$label == "Uncertain"),])
  # predicted uncertain, actual uncertain
  prd_uncert_act_uncertain = nrow(df[(df$class == "Uncertain" & df$label == "Uncertain"),])

  # create confusion matrix
  matrix_data <- c(TP, FN, prd_uncert_act_positive, FP, TN, prd_uncert_act_negative, prd_positive_act_uncertain, prd_negative_act_uncertain, prd_uncert_act_uncertain)
  confusion_matrix<-matrix(matrix_data,nrow = 3,ncol = 3,
                           dimnames = list(c("Prediction Positive ","Prediction Negative", "Prediction Uncertain"),
                                           c("Actual Positive","Actual Negative" , "Actual Uncertain")))

  classified_samples_proportion <- (TP + FN + FP + TN) / sum(confusion_matrix) * 100
  accuracy_amongst_classified_samples <- (TP + TN) / (TP + FN + FP + TN) * 100

  print(paste("Confusion Matrix for ", pathway, " pathway"))
  print("--------------------------------------------------------------")
  print(confusion_matrix)
  print("--------------------------------------------------------------")

  if(display_statistics == 'TRUE'){

    print("Statistics in Confusion Matrix ")
    print("--------------------------------------------------------------")
    print(paste0("Proportion of classified samples: ", format(round(classified_samples_proportion, 2), nsmall = 2)))
    print(paste0("Accuracy amongst classified samples: " , format(round(accuracy_amongst_classified_samples, 2), nsmall = 2)))
    print(paste0("True Positive(TP): " , TP))
    print(paste0("True Negative(TN): " , TN))
    print(paste0("False Negative(FN): " , FN))
    print(paste0("False Positive(FP): " , FP))
    print("--------------------------------------------------------------")
    print(paste0("True Positive Rate(TPR)(sensitivity)(Recall): ",
                 format(round(TP / (TP + FN) * 100 , 2) , nsmall =2)))
    print(paste0("True Negative Rate(TNR)(specificity): ",
                 format(round(TN / (TN + FP) * 100 , 2) , nsmall =2)))
    print(paste0("Precision (Positive predictive value): ",
                 format(round(TP / (TP + FP) * 100 , 2) , nsmall =2)))
    print(paste0("False Positive Rate(FPR): ",
                 format(round(FP / (FP + TN) * 100 ,  2) , nsmall =2)))
    print(paste0("False Negative Rate(FNR): ",
                 format(round(FN / (FN + TP) * 100 ,  2) , nsmall =2)))
    print("--------------------------------------------------------------")
    print(summary(confusion_matrix))
  }

  if(display_roc_curve == TRUE){

    # roc_curve_tmp_df dataframe is created by excluding all rows which are uncertain wither in class(prediction label) or label(real label),
    # So that ROC Curve can be created with  Positive or Negative values after they have been converted to (0,1) binary values


    roc_curve_tmp_df <- df[df$class %in% c("Active", "Inactive")  & df$label %in% c("Negative", "Positive"),]

    roc_curve_tmp_df[,c(2,3)] <- sapply(roc_curve_tmp_df[,c(2,3)], function(x){
      x<-gsub("(Positive|Active)", "1", x)
      x<-gsub("(Negative|Inactive)", "0", x)
      return(as.integer(x))
    })


    labels <- roc_curve_tmp_df$label
    classes <- roc_curve_tmp_df$class

    #Sorting the actual labels by their predicted labels(classes) from 1 to 0
    labels <- labels[order(classes, decreasing=TRUE)]
    #Calculate cumulative True Positive Rate (TPR) and True Negative Rate (TNR) for the ordered actual labels
    roc_curve_df <- data.frame(TPR=cumsum(labels)/sum(labels), FPR=cumsum(!labels)/sum(!labels), labels)

    require(pROC)
    #ROC curve is plotted
    plot(roc(as.logical(roc_curve_tmp_df$label), factor(roc_curve_tmp_df$class, ordered = TRUE), direction="<"),
         col="yellow", main="ROC Curve")
    with(roc_curve_df, points(1 - FPR, TPR, col=1 + labels))

  }

  return(confusion_matrix)
}
